# Minecraft Latent Perception ğŸ®ğŸ§   
*(Component of the Stratus Agent â€“ standalone research module)*

---

## ğŸ“Œ Overview

This project explores **latent representation learning** + **temporal sequence modeling** for Minecraft gameplay frames.

Instead of feeding raw pixels into reinforcement learning models, this module:

1. Captures Minecraft screen frames (via ADB â†’ Android Bedrock, or screen recording).
2. Passes them through a **vision encoder** (CNN / ViT / Autoencoder) â†’ producing a compact latent vector.
3. Over a sequence of frames, an **LSTM (or transformer)** learns the temporal pattern.
4. A classifier / PPO policy predicts the **action happening in the frame**.

> In short: *pixels â†’ latent vector â†’ temporal model â†’ action identification*.

---

## ğŸ§  Why?

Traditional RL learns everything directly from raw pixels.  
That means it has to *simultaneously* figure out:

- Spatial recognition ("what is happening?")
- Temporal consistency ("what just happened before?")
- Action â†’ reward mapping

By splitting responsibilities:

| Module | Job |
|--------|-----|
| **Vision encoder** | Understand the scene (extract features) |
| **LSTM / Transformer** | Understand action sequence over time |
| **Policy / Classifier** | Output what action is happening |

This makes training:
- **faster**
- **more stable**
- **less compute hungry**

---

## ğŸ” Why LSTM / Transformer?

Minecraft actions are not based on a single frame.

Example: breaking a block:
- Frame 1 â†’ crosshair at wood
- Frame 2 â†’ crack stage 1
- Frame 3 â†’ crack stage 2
- Frame 4 â†’ block drops

A single frame can't tell that the agent is *breaking a block*.  
But a sequence of latent vectors reveals the pattern.

### Simple diagram

```
Frame[t-3] Frame[t-2] Frame[t-1] Frame[t]
     â”‚        â”‚        â”‚        â”‚
     â–¼        â–¼        â–¼        â–¼
   Latent   Latent   Latent   Latent
    z1        z2        z3        z4
       â””â”€â”€â”€â”€â”€â”€â”€ into LSTM â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          Predicted Action
```

---

## ğŸš§ Current Scope (MVP)

- âœ… Capture frame â†’ convert to latent vector
- âœ… Sequence model (LSTM/transformer ready)
- â³ Train classifier / PPO to map latent â†’ action

Non-goals (in this repo):
- Crafting logic
- Inventory planning
- High-level reasoning

This repo is intentionally focused on **perception + action recognition**.

---

## ğŸ— Architecture

```
Minecraft Frame (RGB)
        â”‚
        â–¼
  Vision Encoder (CNN/ViT/Autoencoder)
        â”‚ latent vector z_t
        â–¼
 LSTM / Transformer (sequence)
        â”‚
        â–¼
Action Classifier or PPO Policy
```

---

## ğŸ“š Dataset Structure

```
dataset/
   â”œâ”€â”€ frames/
   â”‚     â”œâ”€â”€ clip01/
   â”‚     â”‚      â”œâ”€â”€ frame_0001.png
   â”‚     â”‚      â”œâ”€â”€ frame_0002.png
   â”‚     â”‚      â”œâ”€â”€ ...
   â”œâ”€â”€ labels.csv   # clip_id â†’ action mapping
```

Example `labels.csv`:

```
clip,action
clip01,break_block
clip02,move_forward
clip03,jump
```

---

## ğŸ›  Tech Stack

| Component | Choice |
|----------|--------|
| Frame capture | ADB-to-PC screen fetch |
| Latent encoder | Autoencoder / CNN / Vision Transformer |
| Sequence model | **LSTM / Transformer** |
| Policy (optional) | Stable Baselines PPO |
| Framework | PyTorch |

---

## ğŸ§ª Roadmap

- [ ] Build dataset pipeline
- [ ] Train latent autoencoder
- [ ] Train LSTM over latent sequences
- [ ] Train action classifier or PPO
- [ ] Visualize latent space in 2D/3D (t-SNE / UMAP)

---

## ğŸ¤ Contributions

This is a **public research experiment**.  
Feedback, issues, and PRs are welcome.

---

## ğŸ“„ License

MIT License â€” for open collaboration.

---

## â­ Acknowledgments

Inspired by:
- MineRL
- Vision-based RL research (DeepMind Atari, DreamerV3)
- Curiosity-driven exploration ideas

> â€œUnderstand the world first. Act second.â€
